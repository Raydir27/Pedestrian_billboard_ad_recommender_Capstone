{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071a59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (300, 49)\n",
      "After dropping raw timestamp/IDs (if present): (300, 48)\n",
      "Detected ad categories (count=17):\n",
      "Final feature matrix shape: (300, 32)\n",
      "Final number of classes: 17\n",
      "Train/test shapes: (240, 32) (60, 32)\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's multi_logloss: 3.92667\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 2.84517\n",
      "\n",
      "Test accuracy: 0.0833\n",
      "\n",
      "Saved artifacts to: recommender_artifacts_singular.joblib\n",
      "\n",
      "--- Starting ONNX Conversion ---\n",
      "Model successfully converted and saved as 'lightgbm_model_singular.onnx'\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import warnings\n",
    "# --- New imports for ONNX conversion ---\n",
    "import onnxmltools\n",
    "from onnxmltools import convert_lightgbm\n",
    "from onnxmltools.convert.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt\n",
    "# ------------------------------------\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "# NOTE: Using your uploaded file name\n",
    "DATA_PATH = \"../data/Kiosk_DOOH_Ads_300.csv\"\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "MIN_CLASS_FREQ = 2\n",
    "LGBM_PARAMS = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"n_estimators\": 400,\n",
    "    \"num_leaves\": 64,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "TOP_K = 5\n",
    "ARTIFACT_PATH = \"recommender_artifacts_singular.joblib\"\n",
    "# ------------------------------------------\n",
    "\n",
    "# --------- 1) Load dataset (CSV or Excel) ----------\n",
    "if DATA_PATH.lower().endswith((\".xls\", \".xlsx\")):\n",
    "    df = pd.read_excel(DATA_PATH)\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded dataset:\", df.shape)\n",
    "\n",
    "# ---------- 2) Defensive cleanup: drop obvious ID-like columns ----------\n",
    "for col in [\"Timestamp\", \"ID\", \"Index\"]:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col], errors='ignore')\n",
    "print(\"After dropping raw timestamp/IDs (if present):\", df.shape)\n",
    "\n",
    "# ---------- 3) Detect ad-ranking columns & build LikedTopic ----------\n",
    "ad_prefix = \"What kinds of advertisements do you find most engaging? (1-5 in order of decreasing preference)\"\n",
    "ad_cols = [c for c in df.columns if c.startswith(ad_prefix)]\n",
    "if not ad_cols:\n",
    "    ad_cols = [c for c in df.columns if \"most engaging\" in c.lower() or \"1-5 in order\" in c.lower()]\n",
    "if not ad_cols:\n",
    "    raise RuntimeError(\"Could not detect ad-ranking columns. Please verify your file has the expected ad columns.\")\n",
    "\n",
    "def extract_label(col):\n",
    "    if '[' in col and ']' in col:\n",
    "        return col[col.find('[')+1:col.rfind(']')].strip()\n",
    "    return col\n",
    "\n",
    "ad_labels = [extract_label(c) for c in ad_cols]\n",
    "print(f\"Detected ad categories (count={len(ad_cols)}):\")\n",
    "\n",
    "def get_top_ad(row):\n",
    "    for c, label in zip(ad_cols, ad_labels):\n",
    "        v = row.get(c)\n",
    "        if pd.isna(v): continue\n",
    "        if float(v) == 1.0: return label\n",
    "    return None\n",
    "\n",
    "df['LikedTopic'] = df.apply(get_top_ad, axis=1)\n",
    "df = df.dropna(subset=['LikedTopic']).reset_index(drop=True)\n",
    "\n",
    "# ---------- 4) Detect and process apparel columns ----------\n",
    "upper_cols = [c for c in df.columns if 'Upper' in c and '[' in c]\n",
    "lower_cols = [c for c in df.columns if 'Lower' in c and '[' in c]\n",
    "accessory_cols = [c for c in df.columns if 'accessori' in c.lower()]\n",
    "\n",
    "def pick_choice_onehot(row, cols):\n",
    "    for c in cols:\n",
    "        v = row.get(c)\n",
    "        if pd.isna(v): continue\n",
    "        if float(v) == 1.0: return extract_label(c)\n",
    "    return None\n",
    "\n",
    "def extract_accessory(row, acc_cols):\n",
    "    for c in acc_cols:\n",
    "        if c in row.index and pd.notna(row[c]) and str(row[c]).strip() != '':\n",
    "            return str(row[c]).split(';')[0].strip()\n",
    "    return pick_choice_onehot(row, acc_cols)\n",
    "\n",
    "if upper_cols: df['upper_choice'] = df.apply(lambda r: pick_choice_onehot(r, upper_cols), axis=1)\n",
    "else: df['upper_choice'] = None\n",
    "if lower_cols: df['lower_choice'] = df.apply(lambda r: pick_choice_onehot(r, lower_cols), axis=1)\n",
    "else: df['lower_choice'] = None\n",
    "df['accessory_choice'] = df.apply(lambda r: extract_accessory(r, accessory_cols), axis=1)\n",
    "df = df.dropna(subset=['Age','Gender','LikedTopic']).reset_index(drop=True)\n",
    "df['upper_choice'] = df['upper_choice'].fillna('unknown_upper')\n",
    "df['lower_choice'] = df['lower_choice'].fillna('unknown_lower')\n",
    "df['accessory_choice'] = df['accessory_choice'].fillna('unknown_acc')\n",
    "\n",
    "# ---------- 5) Drop other high-cardinality columns ----------\n",
    "obj_cols = [c for c in df.columns if df[c].dtype == 'object' and c not in ['upper_choice','lower_choice','accessory_choice','LikedTopic','Gender']]\n",
    "high_card = {c: df[c].nunique() for c in obj_cols if df[c].nunique() > 50}\n",
    "if high_card:\n",
    "    df = df.drop(columns=list(high_card.keys()))\n",
    "\n",
    "# --- START of Altered Logic ---\n",
    "\n",
    "# 6) Build feature matrix (X)\n",
    "age_scaler = MinMaxScaler()\n",
    "df['age_norm'] = age_scaler.fit_transform(df[['Age']].astype(float))\n",
    "gender_enc = LabelEncoder()\n",
    "df['gender_enc'] = gender_enc.fit_transform(df['Gender'].astype(str))\n",
    "\n",
    "# Create a single 'Apparel' column by combining all apparel choices\n",
    "df['Apparel'] = df['upper_choice'].str.cat(df[['lower_choice', 'accessory_choice']], sep='|').str.split('|')\n",
    "\n",
    "# Explode the DataFrame to have one row per apparel item\n",
    "df_apparel = df.explode('Apparel')\n",
    "apparel_dummies = pd.get_dummies(df_apparel['Apparel'], prefix='apparel')\n",
    "\n",
    "# Group back by the original index and sum to create the final one-hot encoded matrix\n",
    "# This handles cases where a user has multiple apparel items\n",
    "apparel_features = apparel_dummies.groupby(level=0).sum()\n",
    "\n",
    "# Reindex to ensure alignment with the main DataFrame\n",
    "apparel_features = apparel_features.reindex(df.index, fill_value=0)\n",
    "\n",
    "# Normalize the apparel features as a single block\n",
    "def block_normalize(block_df):\n",
    "    if block_df.shape[1] == 0: return block_df\n",
    "    return block_df.divide(math.sqrt(block_df.shape[1]))\n",
    "\n",
    "apparel_block = block_normalize(apparel_features)\n",
    "\n",
    "X = pd.concat([df[['age_norm','gender_enc']].reset_index(drop=True),\n",
    "               apparel_block.reset_index(drop=True)], axis=1).fillna(0)\n",
    "\n",
    "# --- END of Altered Logic ---\n",
    "\n",
    "# Build target (y)\n",
    "y = df['LikedTopic'].astype(str).values\n",
    "print(\"Final feature matrix shape:\", X.shape)\n",
    "\n",
    "# ---------- 7) Handle rare classes ----------\n",
    "vc = pd.Series(y).value_counts()\n",
    "rare = vc[vc < MIN_CLASS_FREQ].index.tolist()\n",
    "if rare:\n",
    "    df['LikedTopic'] = df['LikedTopic'].apply(lambda x: x if x not in rare else 'other')\n",
    "    y = df['LikedTopic'].astype(str).values\n",
    "label_enc = LabelEncoder()\n",
    "y_enc = label_enc.fit_transform(y)\n",
    "print(\"Final number of classes:\", len(label_enc.classes_))\n",
    "\n",
    "# ---------- 8) Train/test split ----------\n",
    "stratify_param = y_enc if min(Counter(y_enc).values()) >= 2 else None\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_enc, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=stratify_param)\n",
    "print(\"Train/test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# ---------- 9) Sample weights ----------\n",
    "freq = Counter(y_train)\n",
    "total = len(y_train)\n",
    "sample_weight = np.array([total / freq[int(lbl)] for lbl in y_train])\n",
    "\n",
    "# ---------- 10) Train LightGBM model ----------\n",
    "model = lgb.LGBMClassifier(**LGBM_PARAMS)\n",
    "cb = [lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=50)]\n",
    "model.fit(X_train, y_train, sample_weight=sample_weight, eval_set=[(X_test, y_test)], eval_metric='multi_logloss', callbacks=cb)\n",
    "\n",
    "# ---------- 11) Evaluation ----------\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest accuracy: {acc:.4f}\")\n",
    "\n",
    "# ---------- 12) Save artifacts (your original saving) ----------\n",
    "# --- Altered Logic: Now only saving one apparel column list ---\n",
    "artifacts = {\n",
    "    \"model\": model, \"age_scaler\": age_scaler, \"gender_enc\": gender_enc,\n",
    "    \"label_enc\": label_enc, \"apparel_cols\": list(apparel_block.columns),\n",
    "    \"feature_columns\": list(X.columns)\n",
    "}\n",
    "joblib.dump(artifacts, ARTIFACT_PATH)\n",
    "print(f\"\\nSaved artifacts to: {ARTIFACT_PATH}\")\n",
    "# ==================================================================\n",
    "# NEW: Convert the trained model to ONNX\n",
    "# ==================================================================\n",
    "print(\"\\n--- Starting ONNX Conversion ---\")\n",
    "\n",
    "# Define the input signature for the ONNX model based on the final feature matrix X\n",
    "# The shape is [batch_size, num_features]. We use None for batch_size to allow for variable input.\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\n",
    "\n",
    "# Convert the trained LightGBM model ('model') to ONNX format\n",
    "onnx_model = convert_lightgbm(model, initial_types=initial_type)\n",
    "\n",
    "# Save the ONNX model to a file\n",
    "with open(\"../Models/ml_model/lightgbm_model_singular.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Model successfully converted and saved as 'lightgbm_model_singular.onnx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading recommender artifacts...\n",
      "Artifacts loaded successfully.\n",
      "\n",
      "Preprocessing user profile...\n",
      "\n",
      "Making prediction...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute 'get_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMaking prediction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Get the probability distribution for all classes\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_features\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# --- 4. Get and display the top K recommendations ---\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Get the indices of the top K highest probabilities\u001b[39;00m\n\u001b[0;32m     90\u001b[0m top_k_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(probabilities)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][:TOP_K]\n",
      "File \u001b[1;32mc:\\Users\\Rajarshi\\anaconda3\\envs\\capstone\\lib\\site-packages\\lightgbm\\sklearn.py:1627\u001b[0m, in \u001b[0;36mLGBMClassifier.predict_proba\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict_proba\u001b[39m(\n\u001b[0;32m   1616\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1617\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1624\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1625\u001b[0m ):\n\u001b[0;32m   1626\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is set after definition, using a template.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1627\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[0;32m   1628\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   1629\u001b[0m         raw_score\u001b[38;5;241m=\u001b[39mraw_score,\n\u001b[0;32m   1630\u001b[0m         start_iteration\u001b[38;5;241m=\u001b[39mstart_iteration,\n\u001b[0;32m   1631\u001b[0m         num_iteration\u001b[38;5;241m=\u001b[39mnum_iteration,\n\u001b[0;32m   1632\u001b[0m         pred_leaf\u001b[38;5;241m=\u001b[39mpred_leaf,\n\u001b[0;32m   1633\u001b[0m         pred_contrib\u001b[38;5;241m=\u001b[39mpred_contrib,\n\u001b[0;32m   1634\u001b[0m         validate_features\u001b[38;5;241m=\u001b[39mvalidate_features,\n\u001b[0;32m   1635\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1636\u001b[0m     )\n\u001b[0;32m   1637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_objective) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (raw_score \u001b[38;5;129;01mor\u001b[39;00m pred_leaf \u001b[38;5;129;01mor\u001b[39;00m pred_contrib):\n\u001b[0;32m   1638\u001b[0m         _log_warning(\n\u001b[0;32m   1639\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot compute class probabilities or labels \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1640\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdue to the usage of customized objective function.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1641\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning raw scores instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1642\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Rajarshi\\anaconda3\\envs\\capstone\\lib\\site-packages\\lightgbm\\sklearn.py:1125\u001b[0m, in \u001b[0;36mLGBMModel.predict\u001b[1;34m(self, X, raw_score, start_iteration, num_iteration, pred_leaf, pred_contrib, validate_features, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     X \u001b[38;5;241m=\u001b[39m _LGBMValidateData(\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1110\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         ensure_min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# retrieve original params that possibly can be used in both training and prediction\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# and then overwrite them (considering aliases) with params that were passed directly in prediction\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m predict_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alias \u001b[38;5;129;01min\u001b[39;00m _ConfigAliases\u001b[38;5;241m.\u001b[39mget_by_alias(\n\u001b[0;32m   1127\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mkeys(),\n\u001b[0;32m   1135\u001b[0m ):\n\u001b[0;32m   1136\u001b[0m     predict_params\u001b[38;5;241m.\u001b[39mpop(alias, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Rajarshi\\anaconda3\\envs\\capstone\\lib\\site-packages\\lightgbm\\sklearn.py:815\u001b[0m, in \u001b[0;36mLGBMModel._process_params\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process the parameters of this estimator based on its type, parameter aliases, etc.\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;124;03m    Processed parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m stage \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 815\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    817\u001b[0m params\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alias \u001b[38;5;129;01min\u001b[39;00m _ConfigAliases\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Rajarshi\\anaconda3\\envs\\capstone\\lib\\site-packages\\lightgbm\\sklearn.py:768\u001b[0m, in \u001b[0;36mLGBMModel.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get parameters for this estimator.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Based on: https://github.com/dmlc/xgboost/blob/bd92b1c9c0db3e75ec3dfa513e1435d518bb535d/python-package/xgboost/sklearn.py#L941\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# which was based on: https://stackoverflow.com/questions/59248211\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;66;03m#                                             LGBMModel -> BaseEstimator\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m--> 768\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m(deep\u001b[38;5;241m=\u001b[39mdeep)\n\u001b[0;32m    769\u001b[0m cp \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# If the immediate parent defines get_params(), use that.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'super' object has no attribute 'get_params'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "\n",
    "# --- Configuration for testing ---\n",
    "ARTIFACT_PATH = \"recommender_artifacts_singular.joblib\"\n",
    "model = \"../Models/ml_model/lightgbm_model_singular.onnx\"\n",
    "TOP_K = 5\n",
    "\n",
    "# --- User Profile to Test ---\n",
    "# NOTE: This is the user data you want to get a recommendation for.\n",
    "user_age = 58\n",
    "user_gender = \"Female\"\n",
    "# The apparel list should only contain valid items, so we filter out \"None\" and \"unknown_acc\"\n",
    "user_apparel = [\"polo\", \"skirt\", \"spectacles\", \"hat\"]\n",
    "\n",
    "# --- 1. Load the saved artifacts ---\n",
    "print(\"Loading recommender artifacts...\")\n",
    "try:\n",
    "    artifacts = joblib.load(ARTIFACT_PATH)\n",
    "    model = artifacts[\"model\"]\n",
    "    age_scaler = artifacts[\"age_scaler\"]\n",
    "    gender_enc = artifacts[\"gender_enc\"]\n",
    "    label_enc = artifacts[\"label_enc\"]\n",
    "    apparel_cols = artifacts[\"apparel_cols\"]\n",
    "    feature_columns = artifacts[\"feature_columns\"]\n",
    "    print(\"Artifacts loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The artifact file '{ARTIFACT_PATH}' was not found. Please ensure it has been created by the training script.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Preprocess the user profile data ---\n",
    "print(\"\\nPreprocessing user profile...\")\n",
    "\n",
    "# Initialize a DataFrame for the user\n",
    "user_df = pd.DataFrame({\n",
    "    'Age': [user_age],\n",
    "    'Gender': [user_gender]\n",
    "})\n",
    "\n",
    "# Apply age scaling\n",
    "user_df['age_norm'] = age_scaler.transform(user_df[['Age']].astype(float))\n",
    "\n",
    "# Apply gender encoding\n",
    "user_df['gender_enc'] = gender_enc.transform(user_df['Gender'].astype(str))\n",
    "\n",
    "# Create one-hot encoded features for apparel\n",
    "# Initialize a dictionary with all apparel columns set to 0\n",
    "apparel_features = {col: 0 for col in apparel_cols}\n",
    "\n",
    "# Set the value to 1 for each apparel item the user is wearing\n",
    "for item in user_apparel:\n",
    "    # Ensure the item is a valid column in our model's features\n",
    "    if f'apparel_{item}' in apparel_features:\n",
    "        apparel_features[f'apparel_{item}'] = 1\n",
    "\n",
    "# Convert the apparel features dictionary to a DataFrame row\n",
    "apparel_df = pd.DataFrame([apparel_features])\n",
    "\n",
    "# Apply the same block normalization as during training\n",
    "def block_normalize(block_df):\n",
    "    if block_df.shape[1] == 0:\n",
    "        return block_df\n",
    "    return block_df.divide(math.sqrt(block_df.shape[1]))\n",
    "\n",
    "apparel_block = block_normalize(apparel_df)\n",
    "\n",
    "# Concatenate all features into a single DataFrame\n",
    "# We must ensure the columns are in the exact same order as the training data\n",
    "user_features = pd.concat([user_df[['age_norm', 'gender_enc']].reset_index(drop=True), apparel_block.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Reindex the DataFrame to match the feature column order from training\n",
    "user_features = user_features.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "# --- 3. Make a prediction ---\n",
    "print(\"\\nMaking prediction...\")\n",
    "# Get the probability distribution for all classes\n",
    "probabilities = model.predict_proba(user_features)[0]\n",
    "\n",
    "# --- 4. Get and display the top K recommendations ---\n",
    "# Get the indices of the top K highest probabilities\n",
    "top_k_indices = np.argsort(probabilities)[::-1][:TOP_K]\n",
    "\n",
    "print(f\"\\nTop {TOP_K} Recommended Ad Topics:\")\n",
    "for i, index in enumerate(top_k_indices):\n",
    "    # Decode the class index back to the original ad topic name\n",
    "    ad_topic = label_enc.inverse_transform([index])[0]\n",
    "    confidence = probabilities[index]\n",
    "    print(f\"  {i+1}. {ad_topic}: {confidence:.4f} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379cc55",
   "metadata": {},
   "source": [
    " 1. Health Products: 0.0798 confidence\n",
    "  2. Finance/Investing: 0.0663 confidence\n",
    "  3. Beauty & Skincare: 0.0662 confidence\n",
    "  4. Jewelry: 0.0658 confidence\n",
    "  5. Education & Careers: 0.0651 confidence\n",
    "\n",
    "  for age 69, female, polo, skirt, spectacles, hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6b141",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1fd6a84",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
