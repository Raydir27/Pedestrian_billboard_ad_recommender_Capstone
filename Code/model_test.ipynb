{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ae1fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Ultralytics settings reset to default values. This may be due to a possible problem with your settings or a recent ultralytics package update. \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\Rajarshi\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e365d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "model_ped = YOLO(\"../Models/computer_vision_models/pedestrian_detect_model_v8n_24_jul.pt\")\n",
    "model_apparel = YOLO(\"../Models/computer_vision_models/apparel2_v8n_model_13_aug_2025.pt\")\n",
    "\n",
    "def detect_pedestrian(image):\n",
    "    return model_ped(image)\n",
    "\n",
    "def detect_apparel(image):\n",
    "    return model_apparel(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e1cd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pedestrian_crop(pedestrian_results, original_image):\n",
    "    \"\"\"\n",
    "    Crops pedestrians from an image based on detection results and runs\n",
    "    apparel detection on each cropped pedestrian.\n",
    "\n",
    "    Returns:\n",
    "        all_detections: dict with pedestrian_id as key and detected apparel list as value\n",
    "    \"\"\"\n",
    "    all_detections = {}\n",
    "    pedestrian_id = 0\n",
    "\n",
    "    for result in pedestrian_results:\n",
    "        boxes = result.boxes.xyxy.cpu().numpy()\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "            # Crop pedestrian\n",
    "            cropped_pedestrian = original_image[y1:y2, x1:x2]\n",
    "\n",
    "            if cropped_pedestrian.size > 0:\n",
    "                apparel_results = detect_apparel(cropped_pedestrian)\n",
    "\n",
    "                # Extract apparel detections: (class_name, confidence)\n",
    "                apparel_info = []\n",
    "                for a_result in apparel_results:\n",
    "                    for box_a in a_result.boxes:\n",
    "                        cls_id = int(box_a.cls[0])\n",
    "                        conf = float(box_a.conf[0])\n",
    "                        class_name = a_result.names[cls_id]\n",
    "                        apparel_info.append((class_name, conf))\n",
    "\n",
    "                all_detections[pedestrian_id] = {\n",
    "                    'bbox': (x1, y1, x2, y2),\n",
    "                    'apparel': apparel_info\n",
    "                }\n",
    "                pedestrian_id += 1\n",
    "\n",
    "    return all_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3be1456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 14 Pedestrians, 129.2ms\n",
      "Speed: 21.4ms preprocess, 129.2ms inference, 204.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "\n",
      "0: 640x288 1 pant, 128.2ms\n",
      "Speed: 2.5ms preprocess, 128.2ms inference, 77.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "\n",
      "0: 640x224 1 Tshirt, 1 pant, 109.3ms\n",
      "Speed: 3.7ms preprocess, 109.3ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 skirt, 10.4ms\n",
      "Speed: 2.2ms preprocess, 10.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x192 (no detections), 108.9ms\n",
      "Speed: 1.9ms preprocess, 108.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x192 (no detections), 26.5ms\n",
      "Speed: 1.9ms preprocess, 26.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x192 2 shirts, 1 skirt, 17.4ms\n",
      "Speed: 2.1ms preprocess, 17.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x224 (no detections), 12.5ms\n",
      "Speed: 1.6ms preprocess, 12.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x224 1 Tshirt, 12.7ms\n",
      "Speed: 1.9ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 224)\n",
      "\n",
      "0: 640x256 (no detections), 91.6ms\n",
      "Speed: 1.3ms preprocess, 91.6ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x160 (no detections), 126.1ms\n",
      "Speed: 2.4ms preprocess, 126.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 160)\n",
      "\n",
      "0: 640x192 (no detections), 20.3ms\n",
      "Speed: 1.4ms preprocess, 20.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 192)\n",
      "\n",
      "0: 640x256 (no detections), 22.9ms\n",
      "Speed: 4.4ms preprocess, 22.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "0: 640x96 1 Tshirt, 1 suit, 115.5ms\n",
      "Speed: 1.5ms preprocess, 115.5ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 96)\n",
      "\n",
      "0: 640x256 (no detections), 24.8ms\n",
      "Speed: 1.7ms preprocess, 24.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 256)\n",
      "\n",
      "Detected Apparel per Pedestrian:\n",
      "\n",
      " Pedestrian ID Apparel  Confidence\n",
      "             0    pant       0.747\n",
      "             1    pant       0.384\n",
      "             1  Tshirt       0.301\n",
      "             2   skirt       0.657\n",
      "             5   skirt       0.566\n",
      "             5   shirt       0.381\n",
      "             5   shirt       0.289\n",
      "             7  Tshirt       0.339\n",
      "            12    suit       0.488\n",
      "            12  Tshirt       0.255\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    image_path = r\"C:\\Users\\Rajarshi\\Pictures\\capstone_test_images\\test1.jpg\"\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect pedestrians\n",
    "    ped_results = detect_pedestrian(image)\n",
    "\n",
    "    # Annotated image with pedestrian bboxes\n",
    "    annotated_image = ped_results[0].plot()  # YOLO's built-in plotting\n",
    "\n",
    "    # Crop pedestrians and run apparel detection\n",
    "    detections = pedestrian_crop(ped_results, image)\n",
    "\n",
    "    # Display annotated image\n",
    "    cv2.imshow(\"Pedestrian Detection\", annotated_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Prepare table for apparel detections\n",
    "    table_data = []\n",
    "    for pid, info in detections.items():\n",
    "        for apparel_item, conf in info['apparel']:\n",
    "            table_data.append({\n",
    "                \"Pedestrian ID\": pid,\n",
    "                \"Apparel\": apparel_item,\n",
    "                \"Confidence\": round(conf, 3)\n",
    "            })\n",
    "\n",
    "    # Convert to pandas DataFrame for a clean table\n",
    "    df = pd.DataFrame(table_data)\n",
    "    if not df.empty:\n",
    "        print(\"\\nDetected Apparel per Pedestrian:\\n\")\n",
    "        print(df.to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo apparel detected.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045d4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
